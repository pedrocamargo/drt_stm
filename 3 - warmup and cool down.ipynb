{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import sys\n",
    "import random\n",
    "import logging as logger\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.basicConfig(format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "                   level=logger.INFO,\n",
    "                   datefmt='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fldr = \"C:/D/Projects/Vancouver\"\n",
    "outdb = sqlite3.connect(os.path.join(fldr, 'trip_table.sqlite'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All geo layers for the STM were imported to a single Geopackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodelyr = sqlite3.connect(os.path.join(fldr, \"Network/prepared_layer.gpkg\"))\n",
    "nds = pd.read_sql_query(\"SELECT fid, taz FROM nodes where pt_only=0\", nodelyr)\n",
    "tazs = {}\n",
    "for taz in nds.taz.unique():\n",
    "    tazs[taz] = nds[nds.taz==taz].fid.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the trips from the trip table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_sql_query(\"SELECT * FROM trips\", outdb)\n",
    "df.drop(['index'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 502 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = df.assign(from_node=-1)\n",
    "df = df.assign(to_node=-1)\n",
    "df = df.assign(departure_minute=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zone(taz, tazs):\n",
    "    if taz not in tazs:\n",
    "        return -1\n",
    "    return random.choice(tazs[taz])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_profile = pd.read_csv(os.path.join(fldr, 'nhts/dtc_dist.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exact_time(dist):\n",
    "    rdn = np.random.rand(1)[0]\n",
    "#     if rdn > dist.WTTRDFIN.max():\n",
    "#         return -1\n",
    "    return dist.index[np.argmax(dist.WTTRDFIN.values>rdn)] + np.random.rand(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "am_dist = time_profile[450:510]\n",
    "tot_am = am_dist.WTTRDFIN.sum()\n",
    "md_dist = time_profile[720:780]\n",
    "tot_md = md_dist.WTTRDFIN.sum()\n",
    "pm_dist = time_profile[990:1050]\n",
    "tot_pm = pm_dist.WTTRDFIN.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Creates trips on a 30 minutes interval based \n",
    "# XXXXXXX\n",
    "def spatiotemporal_distribution(df, total_original, interval):    \n",
    "    result_df = []\n",
    "    per = 30\n",
    "    for p in range(interval[0], interval[1], per):    \n",
    "        logger.info(\"Processing period {}-{}\".format(p/60,(p + per)/60))\n",
    "        dist = time_profile[p:p + per]\n",
    "        tot_p = dist.WTTRDFIN.sum()\n",
    "        dist = dist.cumsum()[['WTTRDFIN']]\n",
    "        dist.WTTRDFIN /= dist.WTTRDFIN.max()\n",
    "        \n",
    "        pdf = df.sample(frac=tot_p / total_original)\n",
    "        \n",
    "        pdf['departure_minute'] = pdf.apply(lambda row: get_exact_time(dist), axis=1)\n",
    "#         pdf = pdf[pdf.departure_minute > 0]\n",
    "        pdf['from_node'] = pdf.apply(lambda row: get_zone(row['rows'], tazs), axis=1)\n",
    "        pdf['to_node'] = pdf.apply(lambda row: get_zone(row['cols'], tazs), axis=1)\n",
    "        result_df.append(pdf)\n",
    "    return pd.concat(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-22 14:11:39 INFO     Processing period 1.5-2.0\n",
      "2019-05-22 14:11:40 INFO     Processing period 2.0-2.5\n",
      "2019-05-22 14:11:41 INFO     Processing period 2.5-3.0\n",
      "2019-05-22 14:11:41 INFO     Processing period 3.0-3.5\n",
      "2019-05-22 14:11:41 INFO     Processing period 3.5-4.0\n",
      "2019-05-22 14:11:42 INFO     Processing period 4.0-4.5\n",
      "2019-05-22 14:11:42 INFO     Processing period 4.5-5.0\n",
      "2019-05-22 14:11:44 INFO     Processing period 5.0-5.5\n",
      "2019-05-22 14:11:46 INFO     Processing period 5.5-6.0\n",
      "2019-05-22 14:11:50 INFO     Processing period 6.0-6.5\n",
      "2019-05-22 14:11:56 INFO     Processing period 6.5-7.0\n",
      "2019-05-22 14:12:06 INFO     Processing period 7.0-7.5\n",
      "2019-05-22 14:12:24 INFO     Processing period 8.5-9.0\n",
      "2019-05-22 14:12:42 INFO     Processing period 9.0-9.5\n",
      "2019-05-22 14:12:57 INFO     Processing period 9.5-10.0\n",
      "2019-05-22 14:13:12 INFO     Processing period 10.0-10.5\n",
      "2019-05-22 14:13:26 INFO     Processing period 10.5-11.0\n",
      "2019-05-22 14:13:42 INFO     Processing period 11.0-11.5\n",
      "2019-05-22 14:14:01 INFO     Processing period 11.5-12.0\n",
      "2019-05-22 14:14:23 INFO     Processing period 13.0-13.5\n",
      "2019-05-22 14:14:43 INFO     Processing period 13.5-14.0\n",
      "2019-05-22 14:15:02 INFO     Processing period 14.0-14.5\n",
      "2019-05-22 14:15:21 INFO     Processing period 14.5-15.0\n",
      "2019-05-22 14:15:41 INFO     Processing period 15.0-15.5\n",
      "2019-05-22 14:16:03 INFO     Processing period 15.5-16.0\n",
      "2019-05-22 14:16:29 INFO     Processing period 16.0-16.5\n",
      "2019-05-22 14:16:55 INFO     Processing period 17.5-18.0\n",
      "2019-05-22 14:17:08 INFO     Processing period 18.0-18.5\n",
      "2019-05-22 14:17:20 INFO     Processing period 18.5-19.0\n",
      "2019-05-22 14:17:30 INFO     Processing period 19.0-19.5\n",
      "2019-05-22 14:17:39 INFO     Processing period 19.5-20.0\n",
      "2019-05-22 14:17:46 INFO     Processing period 20.0-20.5\n",
      "2019-05-22 14:17:52 INFO     Processing period 20.5-21.0\n",
      "2019-05-22 14:17:58 INFO     Processing period 21.0-21.5\n",
      "2019-05-22 14:18:02 INFO     Processing period 21.5-22.0\n",
      "2019-05-22 14:18:06 INFO     Processing period 22.0-22.5\n",
      "2019-05-22 14:18:09 INFO     Processing period 22.5-23.0\n",
      "2019-05-22 14:18:11 INFO     Processing period 23.0-23.5\n",
      "2019-05-22 14:18:13 INFO     Processing period 23.5-24.0\n"
     ]
    }
   ],
   "source": [
    "# Synthesizes gap periods\n",
    "patch_dfs = []\n",
    "# PRE - AM\n",
    "am_df = df[df['class'].str.contains('Am')]\n",
    "total = tot_am\n",
    "interval = [90, 450]\n",
    "patch_dfs.append(spatiotemporal_distribution(am_df, total, interval))\n",
    "\n",
    "am_md_df = df[df['class'].str.contains('Am|Md')]\n",
    "total = tot_am + tot_md\n",
    "interval = [510, 720]\n",
    "patch_dfs.append(spatiotemporal_distribution(am_md_df, total, interval))\n",
    "\n",
    "md_pm_df = df[df['class'].str.contains('Pm|Md')]\n",
    "total = tot_pm + tot_md\n",
    "interval = [780, 990]\n",
    "patch_dfs.append(spatiotemporal_distribution(md_pm_df, total, interval))\n",
    "\n",
    "pm_df = df[df['class'].str.contains('Pm')]\n",
    "total = tot_pm + tot_md\n",
    "interval = [1050, 1440]\n",
    "patch_dfs.append(spatiotemporal_distribution(pm_df, total, interval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "patch_dfs = pd.concat(patch_dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_dfs.to_sql('distributed_trips', outdb, if_exists='append')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
